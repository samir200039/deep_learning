{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implement a Multi-Layer Perceptron (MLP) to classify handwritten digits from the \n",
    "MNIST dataset. However, restrict the implementation to not use any deep learning \n",
    "library such as TensorFlow or PyTorch, except for data loading. Specifically, you can use \n",
    "libraries or utilities to load and preprocess the MNIST dataset, but all aspects of \n",
    "building and training the neural network should be implemented without relying on \n",
    "external deep learning libraries. Experiment with different architectures by varying the \n",
    "number of hidden layers and neurons to observe their effects on classification \n",
    "performance\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3028984347786023\n",
      "Epoch 10, Loss: 2.295956480918371\n",
      "Epoch 20, Loss: 2.28382324551763\n",
      "Epoch 30, Loss: 2.25715080089639\n",
      "Epoch 40, Loss: 2.199692965005896\n",
      "Epoch 50, Loss: 2.0887234503393053\n",
      "Epoch 60, Loss: 1.9116015997133406\n",
      "Epoch 70, Loss: 1.6829110201371704\n",
      "Epoch 80, Loss: 1.4441953657773008\n",
      "Epoch 90, Loss: 1.235629926979568\n",
      "Test Accuracy: 0.7602\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1] range\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Flatten the images\n",
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_test = x_test.reshape(-1, 28*28)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_one_hot = np.zeros((y_train.size, y_train.max() + 1))\n",
    "y_train_one_hot[np.arange(y_train.size), y_train] = 1\n",
    "\n",
    "y_test_one_hot = np.zeros((y_test.size, y_test.max() + 1))\n",
    "y_test_one_hot[np.arange(y_test.size), y_test] = 1\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.params = self.initialize_parameters()\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        params = {}\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            params[f'W{i+1}'] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i+1]) * 0.01\n",
    "            params[f'b{i+1}'] = np.zeros((1, self.layer_sizes[i+1]))\n",
    "        return params\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def relu_derivative(self, Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    def softmax(self, Z):\n",
    "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        caches = {'A0': X}\n",
    "        A = X\n",
    "        for i in range(len(self.layer_sizes) - 2):\n",
    "            Z = np.dot(A, self.params[f'W{i+1}']) + self.params[f'b{i+1}']\n",
    "            A = self.relu(Z)\n",
    "            caches[f'Z{i+1}'] = Z\n",
    "            caches[f'A{i+1}'] = A\n",
    "        ZL = np.dot(A, self.params[f'W{len(self.layer_sizes)-1}']) + self.params[f'b{len(self.layer_sizes)-1}']\n",
    "        AL = self.softmax(ZL)\n",
    "        caches[f'Z{len(self.layer_sizes)-1}'] = ZL\n",
    "        caches[f'A{len(self.layer_sizes)-1}'] = AL\n",
    "        return AL, caches\n",
    "    \n",
    "    def compute_loss(self, AL, Y):\n",
    "        m = Y.shape[0]\n",
    "        loss = -np.sum(Y * np.log(AL)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward_propagation(self, caches, Y):\n",
    "        grads = {}\n",
    "        L = len(self.layer_sizes) - 1\n",
    "        m = Y.shape[0]\n",
    "        AL = caches[f'A{L}']\n",
    "        dZL = AL - Y\n",
    "        grads[f'dW{L}'] = np.dot(caches[f'A{L-1}'].T, dZL) / m\n",
    "        grads[f'db{L}'] = np.sum(dZL, axis=0, keepdims=True) / m\n",
    "        \n",
    "        for i in reversed(range(1, L)):\n",
    "            dZ = np.dot(dZL, self.params[f'W{i+1}'].T) * self.relu_derivative(caches[f'Z{i}'])\n",
    "            grads[f'dW{i}'] = np.dot(caches[f'A{i-1}'].T, dZ) / m\n",
    "            grads[f'db{i}'] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            dZL = dZ\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    def update_parameters(self, grads, learning_rate):\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            self.params[f'W{i+1}'] -= learning_rate * grads[f'dW{i+1}']\n",
    "            self.params[f'b{i+1}'] -= learning_rate * grads[f'db{i+1}']\n",
    "    \n",
    "    def train(self, X_train, Y_train, learning_rate=0.1, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            AL, caches = self.forward_propagation(X_train)\n",
    "            loss = self.compute_loss(AL, Y_train)\n",
    "            grads = self.backward_propagation(caches, Y_train)\n",
    "            self.update_parameters(grads, learning_rate)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        AL, _ = self.forward_propagation(X)\n",
    "        return np.argmax(AL, axis=1)\n",
    "# Define the architecture of the MLP (e.g., input layer size = 784, one hidden layer with 64 neurons, output layer with 10 neurons)\n",
    "mlp = MLP(layer_sizes=[784, 64, 10])\n",
    "\n",
    "# Train the MLP\n",
    "mlp.train(X_train=x_train, Y_train=y_train_one_hot, learning_rate=0.1, epochs=100)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = mlp.predict(x_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
